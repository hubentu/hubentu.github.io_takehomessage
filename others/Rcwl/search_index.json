[
["index.html", "Bioinformatics tools and pipelines using R and CWL Preface 0.1 R package installation 0.2 System requirements 0.3 Docker 0.4 Structure of the book 0.5 R session information", " Bioinformatics tools and pipelines using R and CWL Qiang Hu Qian Liu 2019-05-23 Preface This short book introduces the R packages, Rcwl and RcwlPipelines, to improve the way of building, managing and running Bioinformatics tools and pipelines. The Rcwl package is built on top of the Common Workflow Language (CWL), and provides a simple and user-friendly way to wrap command line tools into data analysis pipelines in R. The RcwlPipelines package is a collection of Bioinformatics tools and pipelines based on Rcwl. 0.1 R package installation The Rcwl and RcwlPipelines packages can be installed from Bioconductor or Github: BiocManager::install(c(&quot;Rcwl&quot;, &quot;RcwlPipelines&quot;)) # or the development version BiocManager::install(c(&quot;hubentu/Rcwl&quot;, &quot;hubentu/RcwlPipelines&quot;)) To load the packages into R session: library(Rcwl) ## Loading required package: yaml ## Loading required package: S4Vectors ## Loading required package: stats4 ## Loading required package: BiocGenerics ## Loading required package: parallel ## ## Attaching package: &#39;BiocGenerics&#39; ## The following objects are masked from &#39;package:parallel&#39;: ## ## clusterApply, clusterApplyLB, clusterCall, clusterEvalQ, ## clusterExport, clusterMap, parApply, parCapply, parLapply, ## parLapplyLB, parRapply, parSapply, parSapplyLB ## The following objects are masked from &#39;package:stats&#39;: ## ## IQR, mad, sd, var, xtabs ## The following objects are masked from &#39;package:base&#39;: ## ## anyDuplicated, append, as.data.frame, basename, cbind, ## colnames, dirname, do.call, duplicated, eval, evalq, Filter, ## Find, get, grep, grepl, intersect, is.unsorted, lapply, Map, ## mapply, match, mget, order, paste, pmax, pmax.int, pmin, ## pmin.int, Position, rank, rbind, Reduce, rownames, sapply, ## setdiff, sort, table, tapply, union, unique, unsplit, which, ## which.max, which.min ## ## Attaching package: &#39;S4Vectors&#39; ## The following object is masked from &#39;package:base&#39;: ## ## expand.grid ## Registered S3 methods overwritten by &#39;ggplot2&#39;: ## method from ## [.quosures rlang ## c.quosures rlang ## print.quosures rlang library(RcwlPipelines) ## Loading required package: BiocFileCache ## Loading required package: dbplyr 0.2 System requirements In addition to the R packages, the following tools are required to be installed to run the examples in this book. python (&gt;= 2.7) cwltool (&gt;= 1.0.2018) nodejs The cwltool is the reference implementation of the Common Workflow Language, which is used to run the CWL scripts. The nodejs is required when the CWL scripts use JavaScript. You can find instructions to install these tools here: https://github.com/common-workflow-language/cwltool#install https://nodejs.org 0.3 Docker The Docker container simplifies software installation and management, especially for bioinformatics tools/pipelines requiring different runtime environments and library dependencies. A CWL runner can perform this work automatically by pulling the Docker containers and mounting the paths of input files. The Docker requirement is optional, as CWL scripts can also be run locally with all the dependencies pre-installed. 0.4 Structure of the book Introduction Components Build CWL workflows Run approaches Case study 0.5 R session information The R session information for compiling this mannual is shown below: sessionInfo() ## R version 3.6.0 (2019-04-26) ## Platform: x86_64-pc-linux-gnu (64-bit) ## Running under: CentOS release 6.4 (Final) ## ## Matrix products: default ## BLAS: /home/qhu/usr/R-3.6/lib64/R/lib/libRblas.so ## LAPACK: /home/qhu/usr/R-3.6/lib64/R/lib/libRlapack.so ## ## locale: ## [1] LC_CTYPE=en_US.UTF-8 LC_NUMERIC=C ## [3] LC_TIME=en_US.UTF-8 LC_COLLATE=en_US.UTF-8 ## [5] LC_MONETARY=en_US.UTF-8 LC_MESSAGES=en_US.UTF-8 ## [7] LC_PAPER=en_US.UTF-8 LC_NAME=C ## [9] LC_ADDRESS=C LC_TELEPHONE=C ## [11] LC_MEASUREMENT=en_US.UTF-8 LC_IDENTIFICATION=C ## ## attached base packages: ## [1] parallel stats4 stats graphics grDevices utils datasets ## [8] methods base ## ## other attached packages: ## [1] RcwlPipelines_1.0.0 BiocFileCache_1.8.0 dbplyr_1.4.0 ## [4] Rcwl_1.0.1 S4Vectors_0.22.0 BiocGenerics_0.30.0 ## [7] yaml_2.2.0 bookdown_0.9 ## ## loaded via a namespace (and not attached): ## [1] httr_1.4.0 viridis_0.5.1 tidyr_0.8.3 ## [4] bit64_0.9-7 jsonlite_1.6 viridisLite_0.3.0 ## [7] shiny_1.3.2 assertthat_0.2.1 base64url_1.4 ## [10] blob_1.1.1 progress_1.2.0 pillar_1.3.1 ## [13] RSQLite_2.1.1 backports_1.1.4 glue_1.3.1 ## [16] downloader_0.4 digest_0.6.18 RColorBrewer_1.1-2 ## [19] promises_1.0.1 checkmate_1.9.3 colorspace_1.4-1 ## [22] htmltools_0.3.6 httpuv_1.5.1 plyr_1.8.4 ## [25] XML_3.98-1.19 pkgconfig_2.0.2 DiagrammeR_1.0.1 ## [28] purrr_0.3.2 xtable_1.8-4 scales_1.0.0 ## [31] brew_1.0-6 later_0.8.0 BiocParallel_1.18.0 ## [34] tibble_2.1.1 ggplot2_3.1.1 influenceR_0.1.0 ## [37] withr_2.1.2 lazyeval_0.2.2 rgexf_0.15.3 ## [40] magrittr_1.5 crayon_1.3.4 mime_0.6 ## [43] memoise_1.1.0 evaluate_0.13 Rook_1.1-1 ## [46] tools_3.6.0 data.table_1.12.2 prettyunits_1.0.2 ## [49] hms_0.4.2 stringr_1.4.0 munsell_0.5.0 ## [52] compiler_3.6.0 rlang_0.3.4 grid_3.6.0 ## [55] rstudioapi_0.10 rappdirs_0.3.1 htmlwidgets_1.3 ## [58] visNetwork_2.0.6 igraph_1.2.4.1 rmarkdown_1.12 ## [61] gtable_0.3.0 curl_3.3 DBI_1.0.0 ## [64] R6_2.4.0 gridExtra_2.3 knitr_1.22 ## [67] dplyr_0.8.0.1 bit_1.1-14 readr_1.3.1 ## [70] stringi_1.4.3 Rcpp_1.0.1 batchtools_0.9.11 ## [73] tidyselect_0.2.5 xfun_0.6 "],
["intro.html", "Chapter 1 Introduction 1.1 Common Workflow Language 1.2 First example 1.3 Test run", " Chapter 1 Introduction 1.1 Common Workflow Language “The Common Workflow Language (CWL) is a specification for describing analysis workflows and tools in a way that makes them portable and scalable across a variety of software and hardware environments, from workstations to cluster, cloud, and high performance computing (HPC) environments.” https://www.commonwl.org/ To wrap tool and workflow parameters in a standard format Capable of invoking tools from Docker containers Widely used… 1.2 First example The main class and constructor function is cwlParam, which wraps a command line tool and its parameters in a cwlParam object. Let’s start with a simple example, echo hello world. First, we load the package and define the input parameter for “echo”, a string without a prefix. Just an id option required here: input1 &lt;- InputParam(id = &quot;sth&quot;) Second, we create a cwlParam object with baseCommand for the command to execute and InputParamList for the input parameters. echo &lt;- cwlParam(baseCommand = &quot;echo&quot;, inputs = InputParamList(input1)) Now we have a command object to run. Let’s send a string “Hello World!” to the object. Without defining the outputs, it will stream standard output to a temporary file by default. echo$sth &lt;- &quot;Hello World!&quot; echo ## class: cwlParam ## cwlClass: CommandLineTool ## cwlVersion: v1.0 ## baseCommand: echo ## inputs: ## sth (string): Hello World! ## outputs: ## output: ## type: stdout 1.3 Test run The function runCWL is used to run the CWL object by invoking the python tool cwltool. The return will be a list including the command executed, temporary output and logs. The output directory is the current folder by default, but it can be changed by setting outdir option. All standard out and standard error streams can also be printed by setting stderr = \"\". r1 &lt;- runCWL(echo, outdir = tempdir()) ## Final process status is success r1 ## List of length 3 ## names(3): command output logs Here we can check the output to ensure the code did what we expected. r1$output ## [1] &quot;/tmp/RtmptiiapL/eaaeeefce184fc7c6de720cea9b40e3bd9bbd7ad&quot; readLines(r1$output) ## [1] &quot;Hello World!&quot; The executed command was returned in the result list. It shows the command that we have defined to execute. r1$command ## [1] &quot;[job file11ec86a164732.cwl] /tmp/tmpVnQd35$ echo \\\\&quot; ## [2] &quot; &#39;Hello World!&#39; &gt; /tmp/tmpVnQd35/eaaeeefce184fc7c6de720cea9b40e3bd9bbd7ad&quot; The log shows the details of how the cwltool works with CWL scripts. r1$log ## [1] &quot;/home/qhu/.local/bin/cwltool 1.0.20180721142728&quot; ## [2] &quot;Resolved &#39;/tmp/RtmptiiapL/file11ec86a164732.cwl&#39; to &#39;file:///tmp/RtmptiiapL/file11ec86a164732.cwl&#39;&quot; ## [3] &quot;[job file11ec86a164732.cwl] /tmp/tmpVnQd35$ echo \\\\&quot; ## [4] &quot; &#39;Hello World!&#39; &gt; /tmp/tmpVnQd35/eaaeeefce184fc7c6de720cea9b40e3bd9bbd7ad&quot; ## [5] &quot;[job file11ec86a164732.cwl] completed success&quot; ## [6] &quot;{&quot; ## [7] &quot; \\&quot;output\\&quot;: {&quot; ## [8] &quot; \\&quot;checksum\\&quot;: \\&quot;sha1$a0b65939670bc2c010f4d5d6a0b3e4e4590fb92b\\&quot;, &quot; ## [9] &quot; \\&quot;basename\\&quot;: \\&quot;eaaeeefce184fc7c6de720cea9b40e3bd9bbd7ad\\&quot;, &quot; ## [10] &quot; \\&quot;location\\&quot;: \\&quot;file:///tmp/RtmptiiapL/eaaeeefce184fc7c6de720cea9b40e3bd9bbd7ad\\&quot;, &quot; ## [11] &quot; \\&quot;path\\&quot;: \\&quot;/tmp/RtmptiiapL/eaaeeefce184fc7c6de720cea9b40e3bd9bbd7ad\\&quot;, &quot; ## [12] &quot; \\&quot;class\\&quot;: \\&quot;File\\&quot;, &quot; ## [13] &quot; \\&quot;size\\&quot;: 13&quot; ## [14] &quot; }&quot; ## [15] &quot;}&quot; ## [16] &quot;Final process status is success&quot; The runCWL generated two scripts with the default tempfile prefix, the tool wrapper CWL file and the input YML file. The cwltool parses the two scripts and translates them into the command shown before. The output is not defined in the cwlParam object, so the command output was returned to stdout by default. "],
["components.html", "Chapter 2 Components 2.1 Input Parameters 2.2 Output Parameters", " Chapter 2 Components 2.1 Input Parameters 2.1.1 Essential Input parameters For the input parameters, we usually need to define three options, id, type, and prefix. The type can be string, int, long, float, double, and so on. More detail can be found at: https://www.commonwl.org/v1.0/CommandLineTool.html#CWLType. A InputParam constructor is used to define a list of input objects for the command tool, such as “prefix” for the parameter flags and “position” for the paramter orders. More descriptions are available in the CWL specification https://www.commonwl.org/v1.0/CommandLineTool.html#CommandLineBinding. Here is an example from the CWL user guide(http://www.commonwl.org/user_guide/03-input/). We defined the echo with different type of input parameters by InputParam, and the stdout option can be used to caputre the standard output stream into a file: e1 &lt;- InputParam(id = &quot;flag&quot;, type = &quot;boolean&quot;, prefix = &quot;-f&quot;) e2 &lt;- InputParam(id = &quot;string&quot;, type = &quot;string&quot;, prefix = &quot;-s&quot;) e3 &lt;- InputParam(id = &quot;int&quot;, type = &quot;int&quot;, prefix = &quot;-i&quot;) e4 &lt;- InputParam(id = &quot;file&quot;, type = &quot;File&quot;, prefix = &quot;--file=&quot;, separate = FALSE) echoA &lt;- cwlParam(baseCommand = &quot;echo&quot;, inputs = InputParamList(e1, e2, e3, e4), stdout = &quot;output.txt&quot;) Then we give it a try by setting values for the inputs: echoA$flag &lt;- TRUE echoA$string &lt;- &quot;Hello&quot; echoA$int &lt;- 1 tmpfile &lt;- tempfile() write(&quot;World&quot;, tmpfile) echoA$file &lt;- tmpfile r2 &lt;- runCWL(echoA, outdir = tempdir()) ## Final process status is success r2$command ## [1] &quot;[job file11ec859a60c6a.cwl] /tmp/tmp9UR3oF$ echo \\\\&quot; ## [2] &quot; --file=/tmp/tmphnXPHW/stg24c23e3a-091b-488d-a9e6-00aa72403c82/file11ec85bfd402c \\\\&quot; ## [3] &quot; -f \\\\&quot; ## [4] &quot; -i \\\\&quot; ## [5] &quot; 1 \\\\&quot; ## [6] &quot; -s \\\\&quot; ## [7] &quot; Hello &gt; /tmp/tmp9UR3oF/output.txt&quot; 2.1.2 Array Inputs Taking a similar example to the CWL user guide described above, we can define three different type of array as inputs: a1 &lt;- InputParam(id = &quot;A&quot;, type = &quot;string[]&quot;, prefix = &quot;-A&quot;) a2 &lt;- InputParam(id = &quot;B&quot;, type = InputArrayParam(items = &quot;string&quot;, prefix=&quot;-B=&quot;, separate = FALSE)) a3 &lt;- InputParam(id = &quot;C&quot;, type = &quot;string[]&quot;, prefix = &quot;-C=&quot;, itemSeparator = &quot;,&quot;, separate = FALSE) echoB &lt;- cwlParam(baseCommand = &quot;echo&quot;, inputs = InputParamList(a1, a2, a3)) We then set values for the three inputs: echoB$A &lt;- letters[1:3] echoB$B &lt;- letters[4:6] echoB$C &lt;- letters[7:9] echoB ## class: cwlParam ## cwlClass: CommandLineTool ## cwlVersion: v1.0 ## baseCommand: echo ## inputs: ## A (string[]): -A a b c ## B: ## type: array ## prefix: -B= d e f ## C (string[]): -C= g h i ## outputs: ## output: ## type: stdout Now we can check whether the command behaves as we expected: r3 &lt;- runCWL(echoB, outdir = tempdir()) ## Final process status is success r3$command ## [1] &quot;[job file11ec81dc4fe78.cwl] /tmp/tmpLNIjk_$ echo \\\\&quot; ## [2] &quot; -A \\\\&quot; ## [3] &quot; a \\\\&quot; ## [4] &quot; b \\\\&quot; ## [5] &quot; c \\\\&quot; ## [6] &quot; -B=d \\\\&quot; ## [7] &quot; -B=e \\\\&quot; ## [8] &quot; -B=f \\\\&quot; ## [9] &quot; -C=g,h,i &gt; /tmp/tmpLNIjk_/e0f64915a4e821f6b14c2c5cf319532c3260ac65&quot; 2.2 Output Parameters 2.2.1 Capturing Output The outputs, similar to the inputs, is a list of output parameters. Three options, id, type and glob, can be defined. The glob option is used to define a pattern to find files relative to the output directory. Here is an example to unzip a compressed gz file. First, we generate a compressed R script file: zzfil &lt;- file.path(tempdir(), &quot;sample.R.gz&quot;) zz &lt;- gzfile(zzfil, &quot;w&quot;) cat(&quot;sample(1:10, 5)&quot;, file = zz, sep = &quot;\\n&quot;) close(zz) We then define a cwlParam object to use “gzip” to uncompress an input file: ofile &lt;- &quot;sample.R&quot; z1 &lt;- InputParam(id = &quot;uncomp&quot;, type = &quot;boolean&quot;, prefix = &quot;-d&quot;) z2 &lt;- InputParam(id = &quot;out&quot;, type = &quot;boolean&quot;, prefix = &quot;-c&quot;) z3 &lt;- InputParam(id = &quot;zfile&quot;, type = &quot;File&quot;) o1 &lt;- OutputParam(id = &quot;rfile&quot;, type = &quot;File&quot;, glob = ofile) gz &lt;- cwlParam(baseCommand = &quot;gzip&quot;, inputs = InputParamList(z1, z2, z3), outputs = OutputParamList(o1), stdout = ofile) Now the gz object can be used to uncompress the previously generated compressed file: gz$uncomp &lt;- TRUE gz$out &lt;- TRUE gz$zfile &lt;- zzfil r4 &lt;- runCWL(gz, outdir = tempdir()) ## Final process status is success r4$output ## [1] &quot;/tmp/RtmptiiapL/sample.R&quot; Or we can use arguments to set some default parameters: z1 &lt;- InputParam(id = &quot;zfile&quot;, type = &quot;File&quot;) o1 &lt;- OutputParam(id = &quot;rfile&quot;, type = &quot;File&quot;, glob = ofile) Gz &lt;- cwlParam(baseCommand = &quot;gzip&quot;, arguments = list(&quot;-d&quot;, &quot;-c&quot;), inputs = InputParamList(z1), outputs = OutputParamList(o1), stdout = ofile) Gz ## class: cwlParam ## cwlClass: CommandLineTool ## cwlVersion: v1.0 ## baseCommand: gzip ## arguments: -d -c ## inputs: ## zfile (File): ## outputs: ## rfile: ## type: File ## glob: sample.R ## stdout: sample.R Gz$zfile &lt;- zzfil r4a &lt;- runCWL(Gz, outdir = tempdir()) ## Final process status is success To make it for general usage, we can define a pattern with javascript to glob the output, which requires node to be installed in your system PATH: pfile &lt;- &quot;$(inputs.zfile.path.split(&#39;/&#39;).slice(-1)[0].split(&#39;.&#39;).slice(0,-1).join(&#39;.&#39;))&quot; Or we can directly use the CWL built in file property, nameroot: pfile &lt;- &quot;$(inputs.zfile.nameroot)&quot; o2 &lt;- OutputParam(id = &quot;rfile&quot;, type = &quot;File&quot;, glob = pfile) req1 &lt;- list(class = &quot;InlineJavascriptRequirement&quot;) GZ &lt;- cwlParam(baseCommand = c(&quot;gzip&quot;, &quot;-d&quot;, &quot;-c&quot;), requirements = list(), ## assign list(req1) if node installed. inputs = InputParamList(z1), outputs = OutputParamList(o2), stdout = pfile) GZ$zfile &lt;- zzfil r4b &lt;- runCWL(GZ, outdir = tempdir()) ## Final process status is success 2.2.2 Array Outputs We can also capture multiple output files with the glob pattern: a &lt;- InputParam(id = &quot;a&quot;, type = InputArrayParam(items = &quot;string&quot;)) b &lt;- OutputParam(id = &quot;b&quot;, type = OutputArrayParam(items = &quot;File&quot;), glob = &quot;*.txt&quot;) touch &lt;- cwlParam(baseCommand = &quot;touch&quot;, inputs = InputParamList(a), outputs = OutputParamList(b)) touch$a &lt;- c(&quot;a.txt&quot;, &quot;b.gz&quot;, &quot;c.txt&quot;) r5 &lt;- runCWL(touch, outdir = tempdir()) ## Final process status is success r5$output ## [1] &quot;/tmp/RtmptiiapL/a.txt&quot; &quot;/tmp/RtmptiiapL/c.txt&quot; 2.2.3 Standard output Usually, the stdout option is a string or an expression of output file name from the command tool. The command’s standard output stream will be captured into a file written to the designated output directory. When the stdout field is defined, an output parameter with the type of “stdout” should be also assigned with no “outputBinding” set. An example for command tool “cat” is defined with stdout field in the output, with the name passed from the input parameter “p2”: ## define Cat p1 &lt;- InputParam(id = &quot;infiles&quot;, type = &quot;File[]&quot;) p2 &lt;- InputParam(id = &quot;outfile&quot;, type = &quot;string&quot;, default = &quot;catout.txt&quot;, position = -1) Cat &lt;- cwlParam(baseCommand = &quot;cat&quot;, inputs = InputParamList(p1, p2), stdout = &quot;$(inputs.outfile)&quot;) ## assign inputs afile &lt;- file.path(tempdir(), &quot;a.txt&quot;) bfile &lt;- file.path(tempdir(), &quot;b.txt&quot;) write(&quot;a&quot;, afile) write(&quot;b&quot;, bfile) Cat$infiles &lt;- list(afile, bfile) ## run r6 &lt;- runCWL(Cat, outdir = tempdir()) ## Final process status is success r6$command ## [1] &quot;[job file11ec83f64de8c.cwl] /tmp/tmp0HtXR2$ cat \\\\&quot; ## [2] &quot; /tmp/tmpQaej3I/stg4b7cc907-eb3f-4c5d-a93f-c1fafb798ce8/a.txt \\\\&quot; ## [3] &quot; /tmp/tmpQaej3I/stga54d178a-bc97-41b2-8493-b49ca60a5fc5/b.txt &gt; /tmp/tmp0HtXR2/catout.txt&quot; In this example, we used the parameter “p2” to pass the name to the standard output. In the InputParam of “p2”, the position is assigned to a negative value (-1), which means the parameters will not be used in the command and only uses for passing variable. To write the “Cat” tool to a CWL file, the “inputBinding” field will be skipped for this parameter. "],
["writing-pipeline.html", "Chapter 3 Writing Pipeline 3.1 Scattering pipeline 3.2 Pipeline plot", " Chapter 3 Writing Pipeline We can connect multiple tools into a pipeline. Here is an example to uncompress an R script and execute it with Rscript. We first define a simple Rscript tool without using docker: d1 &lt;- InputParam(id = &quot;rfile&quot;, type = &quot;File&quot;) Rs &lt;- cwlParam(baseCommand = &quot;/usr/bin/Rscript&quot;, inputs = InputParamList(d1)) Rs ## class: cwlParam ## cwlClass: CommandLineTool ## cwlVersion: v1.0 ## baseCommand: /usr/bin/Rscript ## inputs: ## rfile (File): ## outputs: ## output: ## type: stdout Here is the test run: Rs$rfile &lt;- r4$output tres &lt;- runCWL(Rs, outdir = tempdir()) ## Final process status is success readLines(tres$output) ## [1] &quot;[1] 2 7 4 3 9&quot; The pipeline includes two steps, decompressed by GZ and compiled by Rs. The input file is a compressed file and the output file would be the output Rout from Rs. First we need to define the direct inputs and outputs from GZ and Rs, respectively: i1 &lt;- InputParam(id = &quot;cwl_zfile&quot;, type = &quot;File&quot;) o1 &lt;- OutputParam(id = &quot;cwl_cout&quot;, type = &quot;File&quot;, outputSource = &quot;Compile/output&quot;) For the input “cwl_zifle”, it refers to the GZ input zfile. The output “cwl_cout” will be the outcome of Rs output Rout. The cwlStepParam is used to define inputs and outputs, and the Step function is used to define the two steps. The run option refers to the corresponding cwlParam object and the In option should be linked to the input parameters defined by cwlStepParam. At the end, we use + to connect all steps: cwl &lt;- cwlStepParam(inputs = InputParamList(i1), outputs = OutputParamList(o1)) s1 &lt;- Step(id = &quot;Uncomp&quot;, run = GZ, In = list(zfile = &quot;cwl_zfile&quot;)) s2 &lt;- Step(id = &quot;Compile&quot;, run = Rs, In = list(rfile = &quot;Uncomp/rfile&quot;)) cwl &lt;- cwl + s1 + s2 cwl ## class: cwlStepParam ## cwlClass: Workflow ## cwlVersion: v1.0 ## inputs: ## cwl_zfile (File): ## outputs: ## cwl_cout: ## type: File ## outputSource: Compile/output ## steps: ## Uncomp: ## run: Uncomp.cwl ## zfile: cwl_zfile ## out: rfile ## Compile: ## run: Compile.cwl ## rfile: Uncomp/rfile ## out: output Let’s run the pipeline: cwl$cwl_zfile &lt;- zzfil r7 &lt;- runCWL(cwl, outdir = tempdir()) ## Final process status is success readLines(r7$output) ## [1] &quot;[1] 10 5 1 8 2&quot; 3.1 Scattering pipeline The scattering feature can specify the associated workflow steps or subworkflows to execute separately over a list of input elements. To use this feature, ScatterFeatureRequirement must be specified in the workflow requirement. Different scatter methods can be used in the associated steps to decompose the input into a discrete set of jobs. More details can be found at: https://www.commonwl.org/v1.0/Workflow.html#WorkflowStep. Here is an example to execute multiple R scripts. First, we need to set the input and output types to be an array of “File”, and add the requirments. In the “Compile” step, the scattering input is required to be set with the scatter option: i2 &lt;- InputParam(id = &quot;cwl_rfiles&quot;, type = &quot;File[]&quot;) o2 &lt;- OutputParam(id = &quot;cwl_couts&quot;, type = &quot;File[]&quot;, outputSource = &quot;Compile/output&quot;) req1 &lt;- list(class = &quot;ScatterFeatureRequirement&quot;) cwl2 &lt;- cwlStepParam(requirements = list(req1), inputs = InputParamList(i2), outputs = OutputParamList(o2)) s1 &lt;- Step(id = &quot;Compile&quot;, run = Rs, In = list(rfile = &quot;cwl_rfiles&quot;), scatter = &quot;rfile&quot;) cwl2 &lt;- cwl2 + s1 cwl2 ## class: cwlStepParam ## cwlClass: Workflow ## cwlVersion: v1.0 ## requirements: ## - class: ScatterFeatureRequirement ## inputs: ## cwl_rfiles (File[]): ## outputs: ## cwl_couts: ## type: File[] ## outputSource: Compile/output ## steps: ## Compile: ## run: Compile.cwl ## rfile: cwl_rfiles ## out: output ## scatter: rfile Now multiple R scripts can be assigned to the workflow inputs and executed: cwl2$cwl_rfiles &lt;- c(r4b$output, r4b$output) r8 &lt;- runCWL(cwl2, outdir = tempdir()) ## Final process status is success r8$output ## [1] &quot;/tmp/RtmptiiapL/7ad51e3662ad61d30a8d646578de58856acdd94c&quot; ## [2] &quot;/tmp/RtmptiiapL/7ad51e3662ad61d30a8d646578de58856acdd94c&quot; 3.2 Pipeline plot The function plotCWL can be used to visualize the relationship of inputs, outputs and the components for a tool or pipeline: plotCWL(cwl) "],
["run-approaches.html", "Chapter 4 Run approaches 4.1 Running Tools in Docker 4.2 Running Tools in Cluster server 4.3 Web Application", " Chapter 4 Run approaches 4.1 Running Tools in Docker The CWL can work with docker to simplify your software management and communicate files between host and container. The docker container can be defined by the hints or requirements option: d1 &lt;- InputParam(id = &quot;rfile&quot;, type = &quot;File&quot;) req1 &lt;- list(class = &quot;DockerRequirement&quot;, dockerPull = &quot;r-base&quot;) doc &lt;- cwlParam(baseCommand = &quot;Rscript&quot;, inputs = InputParamList(d1), stdout = &quot;output.txt&quot;, hints = list(req1)) doc$rfile &lt;- r4$output r6 &lt;- runCWL(doc) The tools defined with docker requirements can also be run locally by disabling the docker option. In case your Rscript depends some local libraries to run, an option from cwltools, “–preserve-entire-environment”, can be used to pass all environment variables. r6a &lt;- runCWL(doc, docker = FALSE, outdir = tempdir(), Args = &quot;--preserve-entire-environment&quot;) ## Final process status is success 4.2 Running Tools in Cluster server The CWL can also work in high performance clusters with batch-queuing system, such as SGE, PBS, SLURM and so on, using the Bioconductor package BiocParallel. Here is an example to submit jobs with “Multiicore” and “SGE”, seperately: library(BiocParallel) sth.list &lt;- as.list(LETTERS) names(sth.list) &lt;- LETTERS ## submit with mutlicore result1 &lt;- runCWLBatch(cwl = echo, outdir = tempdir(), inputList = list(sth = sth.list), BPPARAM = MulticoreParam(26)) ## submit with SGE result2 &lt;- runCWLBatch(cwl = echo, outdir = tempdir(), inputList = list(sth = sth.list), BPPARAM = BatchtoolsParam(workers = 26, cluster = &quot;sge&quot;, resources = list(queue = &quot;all.q&quot;))) A more detailed example can be found (https://hubentu.github.io/others/Rcwl_RNASeq.html). 4.3 Web Application 4.3.1 cwlParam example Here we build a tool with different types of input parameters: e1 &lt;- InputParam(id = &quot;flag&quot;, type = &quot;boolean&quot;, prefix = &quot;-f&quot;, doc = &quot;boolean flag&quot;) e2 &lt;- InputParam(id = &quot;string&quot;, type = &quot;string&quot;, prefix = &quot;-s&quot;) e3 &lt;- InputParam(id = &quot;option&quot;, type = &quot;string&quot;, prefix = &quot;-o&quot;) e4 &lt;- InputParam(id = &quot;int&quot;, type = &quot;int&quot;, prefix = &quot;-i&quot;, default = 123) e5 &lt;- InputParam(id = &quot;file&quot;, type = &quot;File&quot;, prefix = &quot;--file=&quot;, separate = FALSE) e6 &lt;- InputParam(id = &quot;array&quot;, type = &quot;string[]&quot;, prefix = &quot;-A&quot;, doc = &quot;separated by comma&quot;) mulEcho &lt;- cwlParam(baseCommand = &quot;echo&quot;, id = &quot;mulEcho&quot;, label = &quot;Test parameter types&quot;, inputs = InputParamList(e1, e2, e3, e4, e5, e6), stdout = &quot;output.txt&quot;) mulEcho ## class: cwlParam ## cwlClass: CommandLineTool ## cwlVersion: v1.0 ## baseCommand: echo ## inputs: ## flag (boolean): -f ## string (string): -s ## option (string): -o ## int (int): -i 123 ## file (File): --file= ## array (string[]): -A ## outputs: ## output: ## type: stdout ## stdout: output.txt 4.3.2 cwlParam to Shiny App Some input parameters can be predefined in a list, which will be converted to selected options in the webapp. An upload parameter can be used to generate an upload interface for the file type option. If FALSE is set for upload, the upload field will be text input (file path) instead of file input. inputList &lt;- list(option = c(&quot;option1&quot;, &quot;option2&quot;)) app &lt;- cwlShiny(mulEcho, inputList, upload = TRUE) runApp(app) shinyApp "],
["application.html", "Chapter 5 Application 5.1 RcwlPipelines tools 5.2 RcwlPipelines summary 5.3 DNASeq alignment pipeline 5.4 RNASeq pipeline 5.5 MC3 somatic variant calling pipeline 5.6 GATK4 germline variant calling pipeline", " Chapter 5 Application 5.1 RcwlPipelines tools 5.1.1 Rcwl scripts The R scripts to build the CWL tools and pipelines based on the Rcwl package are stored in the “tools” and “pipelines” folder, respectively. The function cwlTools can be used to catalog the available scripts, where the cachePath can be your existing cache directory or a new folder: tools &lt;- cwlTools(cachePath = tempdir()) tools ## class: BiocFileCache ## bfccache: /tmp/RtmptiiapL ## bfccount: 36 ## For more information see: bfcinfo() or bfcquery() The full paths can be pulled from the “fpath” column: library(dplyr) ## ## Attaching package: &#39;dplyr&#39; ## The following objects are masked from &#39;package:dbplyr&#39;: ## ## ident, sql ## The following objects are masked from &#39;package:S4Vectors&#39;: ## ## first, intersect, rename, setdiff, setequal, union ## The following objects are masked from &#39;package:BiocGenerics&#39;: ## ## combine, intersect, setdiff, union ## The following objects are masked from &#39;package:stats&#39;: ## ## filter, lag ## The following objects are masked from &#39;package:base&#39;: ## ## intersect, setdiff, setequal, union bfcinfo(tools) %&gt;% select(rname, fpath) ## # A tibble: 36 x 2 ## rname fpath ## &lt;chr&gt; &lt;chr&gt; ## 1 blastn /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## 2 bowtie2_align /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## 3 bowtie2_build /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## 4 bwa_index /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## 5 bwa /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## 6 cutadapt /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## 7 fastQC /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## 8 featureCounts /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## 9 geneBody_cover… /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## 10 genePredToBed /home/qhu/usr/R-3.6/lib64/R/library/RcwlPipelines/tools… ## # … with 26 more rows 5.1.2 Build a pipeline A pipline can be built by utilizing the tools cataloged by the tools. For example, a simple alignment pipelines with mapping and marking duplicates can be built from the tools. First, we load the required tools, bwa, samtools and picard markduplicates: scripts &lt;- bfcinfo(tools) %&gt;% filter(rname %in% c(&quot;bwa&quot;, &quot;samtools_samTobam&quot;, &quot;samtools_sortBam&quot;, &quot;samtools_index&quot;, &quot;markdup&quot;)) %&gt;% pull(rpath) invisible(sapply(scripts, source)) Next, we define the input parameters: p1 &lt;- InputParam(id = &quot;threads&quot;, type = &quot;int&quot;) p2 &lt;- InputParam(id = &quot;RG&quot;, type = &quot;string&quot;) p3 &lt;- InputParam(id = &quot;Ref&quot;, type = &quot;string&quot;) p4 &lt;- InputParam(id = &quot;FQ1&quot;, type = &quot;File&quot;) p5 &lt;- InputParam(id = &quot;FQ2&quot;, type = &quot;File?&quot;) Then we define the pipeline steps, from raw fastqs to duplicates marked alignments: ## bwa s1 &lt;- Step(id = &quot;bwa&quot;, run = bwa, In = list(threads = &quot;threads&quot;, RG = &quot;RG&quot;, Ref = &quot;Ref&quot;, FQ1 = &quot;FQ1&quot;, FQ2 = &quot;FQ2&quot;)) ## sam to bam s2 &lt;- Step(id = &quot;sam2bam&quot;, run = sam2bam, In = list(sam = &quot;bwa/sam&quot;)) ## sort bam s3 &lt;- Step(id = &quot;sortBam&quot;, run = sortBam, In = list(bam = &quot;sam2bam/bam&quot;)) ## mark duplicates s4 &lt;- Step(id = &quot;markdup&quot;, run = markdup, In = list(ibam = &quot;sortBam/sbam&quot;, obam = list( valueFrom=&quot;$(inputs.ibam.nameroot).mdup.bam&quot;), matrix = list( valueFrom=&quot;$(inputs.ibam.nameroot).markdup.txt&quot;))) ## index bam s5 &lt;- Step(id = &quot;idxBam&quot;, run = samtools_index, In = list(bam = &quot;markdup/mBam&quot;)) Last, we define the outputs and connect the steps to a new pipeline: req1 &lt;- list(class = &quot;StepInputExpressionRequirement&quot;) req2 &lt;- list(class = &quot;InlineJavascriptRequirement&quot;) ## outputs o1 &lt;- OutputParam(id = &quot;Bam&quot;, type = &quot;File&quot;, outputSource = &quot;markdup/mBam&quot;) o2 &lt;- OutputParam(id = &quot;Idx&quot;, type = &quot;File&quot;, outputSource = &quot;idxBam/idx&quot;) ## stepParam Align &lt;- cwlStepParam(requirements = list(req1, req2), inputs = InputParamList(p1, p2, p3, p4, p5), outputs = OutputParamList(o1, o2)) ## build pipeline Align &lt;- Align + s1 + s2 + s3 + s4 + s5 The pipeline is ready for use. We can plot the pipeline with plotCWL from the Rcwl package. plotCWL(Align) 5.2 RcwlPipelines summary So far we have built 4 major pipelines in this package. Here is a brief introduction to these 4 pipelines. More pipelines and tools are expected to be included in the future. 5.3 DNASeq alignment pipeline The pipeline can be used to preprocess DNA sequences in fastq format. It can take paired fastqs and read groups from multiple batches as input. data(alignMerge) inputs(alignMerge) ## List of length 6 ## names(6): idBam RG threads Ref FQ1s FQ2s The pipeline includes two steps and several jobs will be run in each step. bwaAlign: bwa alignment by read groups: runs(runs(alignMerge)[[1]]) ## List of length 4 ## names(4): bwa sam2bam sortBam idxBam bwa: To align fastqs and read groups to reference genome with bwa. sam2bam: To convert the alignments in “sam” format to “bam” format with samtools. sortBam: To sort the “bam” file by coordinates with samtools. idxBam: To index “bam” file with samtools. mergeBamDup: To merge by samples and mark duplicates: runs(runs(alignMerge)[[2]]) ## List of length 4 ## names(4): mergeBam markdup samtools_index samtools_flagstat mergeBam: To merge bam files from multiple batches with picard. markdup: To mark duplicates with picard. samtools_index: To index bam file with samtools. samtools_flagstat: To summarize flags in bam with samtools. The final bam files with duplicates marked, bam index, duplicates matrix, and flag statistics summary will be in the output folder. outputs(alignMerge) ## List of length 4 ## names(4): oBam matrix Idx stat Here is the short summary and steps plot: short(alignMerge) ## inputs: ## - idBam ## - RG ## - threads ## - Ref ## - FQ1s ## - FQ2s ## outputs: ## - oBam ## - matrix ## - Idx ## - stat ## steps: ## - bwaAlign ## - mergeBamDup plotCWL(alignMerge) 5.3.1 Prepare data Here is a simple example of two samples. The “sample1” have two lanes of sequences and the “sample2” has only one pair of reads. The lists of reads1 fq1, reads2 fq2, read groups and output BAM names are defined in the inputList. The reference genome and number of threads to run the job are defined in the shared options, paramList: fq1 &lt;- list(sample1 = list(&quot;apps/DNASeq/data/fq1_1.fq&quot;, &quot;apps/DNASeq/data/fq2_1.fq&quot;), sample2 = list(&quot;apps/DNASeq/data/fq1_1.fq&quot;)) fq2 &lt;- list(sample1 = list(&quot;apps/DNASeq/data/fq1_2.fq&quot;, &quot;apps/DNASeq/data/fq2_2.fq&quot;), sample2 = list(&quot;apps/DNASeq/data/fq1_2.fq&quot;)) rgs &lt;- list(sample1 = list(&quot;@RG\\\\tID:sample1.1\\\\tSM:sample1&quot;, &quot;@RG\\\\tID:sample1.2\\\\tSM:sample1&quot;), sample2 = list(&quot;@RG\\\\tID:sample2.1\\\\tSM:sample2&quot;)) samples &lt;- list(sample1 = &quot;sample1.bam&quot;, sample2 = &quot;sample2.bam&quot;) inputList &lt;- list(idBam = samples, RG= rgs, FQ1s = fq1, FQ2s = fq2) paramList &lt;- list(threads = 2, Ref = &quot;apps/data/hs37d5.fa&quot;) 5.3.2 Run in cluster res &lt;- runCWLBatch(alignMerge, outdir = &quot;output/BAM&quot;, inputList = inputList, paramList = paramList, BPPARAM = BatchtoolsParam(workers = 2, cluster = &quot;sge&quot;, template = &quot;apps/data/sge.tmpl&quot;, resources = list(jobname=&quot;bwa&quot;, threads = 2, queue = &quot;all.q&quot;))) List outputs: dir(&quot;apps/DNASeq/output/BAM/sample1&quot;) ## [1] &quot;sample1.bam&quot; &quot;sample1.bam.bai&quot; &quot;sample1.flagstat.txt&quot; ## [4] &quot;sample1.markdup.txt&quot; 5.4 RNASeq pipeline The pipeline was built with reads quality summary, STAR alignment, quantification by featureCounts and RSeQC quality control. Here are the inputs: data(rnaseq_Sf) inputs(rnaseq_Sf) ## List of length 5 ## names(5): in_seqfiles in_prefix in_genomeDir in_GTFfile in_runThreadN The pipeline includes 6 steps: fastqc: To run quality summary for raw fastqs with fastqc. STAR: To align fastqs with STAR. samtools_index: To index aligned bam file. samtools_flagstat: To summarize alignment flags. featureCounts: To quantify gene abundances. RSeQC: Several steps included. gtfToGenePred: To convert GTF annotation to “genePred” format. genePredToBed: To convert “genePred” annotation to “bed” format. r_distribution: To summarize reads distribution over genome features. gCoverage: To summarize read coverage over gene body. The outputs and logs from alignment, quantification and QC steps are collected together into the output folder. A final QC report could be generated by multiqc, which is also available in the data package. Here are the short summary and steps plot: short(rnaseq_Sf) ## inputs: ## - in_seqfiles ## - in_prefix ## - in_genomeDir ## - in_GTFfile ## - in_runThreadN ## outputs: ## - out_fastqc ## - out_BAM ## - out_Log ## - out_Count ## - out_idx ## - out_stat ## - out_count ## - out_distribution ## - out_gCovP ## - out_gCovT ## steps: ## - fastqc ## - STAR ## - samtools_index ## - samtools_flagstat ## - featureCounts ## - RSeQC plotCWL(rnaseq_Sf) 5.4.1 Prepare data An RNASeq test data set can be downloaded from genomedata, which includes paired-end fastqs for 6 samples. download.file(&quot;http://genomedata.org/rnaseq-tutorial/HBR_UHR_ERCC_ds_5pc.tar&quot;, &quot;apps/RNASeq/data/HBR_UHR_ERCC_ds_5pc.tar) untar(&quot;apps/RNASeq/data/HBR_UHR_ERCC_ds_5pc.tar&quot;, exdir = &quot;apps/RNASeq/data/&quot;) The input data must be in a named list, with the same names as the input list of the pipeline. For this pipeline, 5 inputs are required to be set, including in_seqfiles, in_prefix, in_genomeDir, in_GTFfile and in_runThreadN. There are two different input lists, inputList and paramList. The inputList is used to define the inputs for each sample and will be submitted to different cluster nodes. The paramList is used to define parameters which are shared in all jobs. Two following inputs should be listed in inputList: in_seqfiles: A list with the fastq files of each sample in each element. The names of the list are also required to be defined and can be the sample IDs. The length of the list will be the same as the number of samples, thus the list will be defined to inputList and assigned to different nodes for parallel computing. in_prefix is the same as in_seqfiles, which defines a list of sample IDs. files &lt;- normalizePath(list.files(&quot;apps/RNASeq/data/&quot;, &quot;.gz&quot;, full.names = TRUE)) files &lt;- tapply(files, substring(basename(files), 1, 8), as.list) inputList &lt;- list(in_seqfiles = files, in_prefix = as.list(names(files))) These 3 parameter will be defined in paramList: in_genomeDir: The reference genome indexes for STAR. in_GTFfile: The gene annotation file in GTF format. in_runThreadN: The number of threads to run for each job. paramList &lt;- list( in_genomeDir = &quot;apps/data/GRCh38_100/&quot;, in_GTFfile = &quot;apps/data/gencode.v25.annotation.gtf&quot;, in_runThreadN = 4 ) In some cases, we need to modify the default arguments in some steps of a pipeline. For example, arguments(rnaseq_Sf, &quot;STAR&quot;)[[2]] &lt;- &quot;2&quot; head(arguments(rnaseq_Sf, &quot;STAR&quot;)) ## [[1]] ## [1] &quot;--outFilterMultimapNmax&quot; ## ## [[2]] ## [1] &quot;2&quot; ## ## [[3]] ## [1] &quot;--outSAMunmapped&quot; ## ## [[4]] ## [1] &quot;Within&quot; ## ## [[5]] ## [1] &quot;--outFilterMismatchNmax&quot; ## ## [[6]] ## [1] &quot;2&quot; 5.4.2 Submit pipeline with SGE The function runCWLBatch is used to submit the pipeline to cluster server. In addition to defining inputList and paramList, we need to define parallel parameters from the BiocParallel package. Here is an example where we use “sge” to submit the jobs. The “sge” template is a bash script with some predefined parameters for “qsub”. The nodes queue name and number of slots/threads are variables from the template and can be assigned by the resources list. res &lt;- runCWLBatch(cwl = rnaseq_Sf, outdir = &quot;apps/RNASeq/output/&quot;, inputList = inputList, paramList = paramList, BPPARAM = BatchtoolsParam( workers = lengths(inputList)[1], cluster = &quot;sge&quot;, template = &quot;apps/data/sge_centos7.tmpl&quot;, resources = list(queue = &quot;centos7.q&quot;, threads = 4))) That’s it! The fastqc files of each sample will be submitted to different nodes to run the whole pipeline automatically. All the results have been collected to output directory of each sample. For example, dir(&quot;apps/RNASeq/output/HBR_Rep1&quot;) ## [1] &quot;HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read1_fastqc.zip&quot; ## [2] &quot;HBR_Rep1_ERCC-Mix2_Build37-ErccTranscripts-chr22.read2_fastqc.zip&quot; ## [3] &quot;HBR_Rep1Aligned.sortedByCoord.out.bam&quot; ## [4] &quot;HBR_Rep1Aligned.sortedByCoord.out.bam.bai&quot; ## [5] &quot;HBR_Rep1Aligned.sortedByCoord.out.distribution.txt&quot; ## [6] &quot;HBR_Rep1Aligned.sortedByCoord.out.featureCounts.txt&quot; ## [7] &quot;HBR_Rep1Aligned.sortedByCoord.out.featureCounts.txt.summary&quot; ## [8] &quot;HBR_Rep1Aligned.sortedByCoord.out.flagstat.txt&quot; ## [9] &quot;HBR_Rep1Aligned.sortedByCoord.out.geneBodyCoverage.curves.pdf&quot; ## [10] &quot;HBR_Rep1Aligned.sortedByCoord.out.geneBodyCoverage.txt&quot; ## [11] &quot;HBR_Rep1Log.final.out&quot; ## [12] &quot;HBR_Rep1ReadsPerGene.out.tab&quot; 5.4.3 Summarize QC The tool “multiqc” can aggregate results from the multiple outputs of the pipeline and generate a single page report, which also was implemented in the RcwlPipelines package: data(multiqc) multiqc$dir &lt;- &quot;apps/RNASeq/output&quot; multiqc We can also run the tool using Rcwl locally with the option docker = TRUE: runCWL(multiqc, stderr = &quot;&quot;, Args = &quot;--preserve-entire-environment&quot;, docker = FALSE) Here we got the QC report: https://hubentu.github.io/others/multiqc_report.html 5.5 MC3 somatic variant calling pipeline The Multi-Center Mutation Calling in Multiple Cancers project (MC3) pipeline was developed by TCGA to generate a comprehensive encyclopedia of somatic mutation calls. MC3 works by applying an ensemble of seven mutation-calling algorithms with scoring and artifact filtering. More details can be found in this paper: Scalable Open Science Approach for Mutation Calling of Tumor Exomes Using Multiple Genomic Pipelines The mc3 pipeline is available at https://github.com/OpenGenomics/mc3. All required software have been deployed in cloud with docker. The pipeline has been imported and contained in the RcwlPipelines pacakge, which contains two major steps (markID step was removed): Call variants by 7 pipelines Merge VCF and convert to MAF The steps of the pipeline was built on the CWL files from its github repository, which were also contained in the package. Thereforce, we need to load the pipleine by sourcing it from the script. bfcinfo(tools) %&gt;% filter(rname == &quot;mc3&quot;) %&gt;% pull(rpath) %&gt;% source short(mc3) ## inputs: ## - tumorID ## - normalID ## - tumor ## - normal ## - bed_file ## - centromere ## - cosmic ## - dbsnp ## - refFasta ## - vepData ## outputs: ## - outmaf ## - outvcf ## steps: ## - call_variants ## - convert plotCWL(mc3) Two steps are included. 1. call_variants: To call variants by 7 pipelines: callVar &lt;- readCWL(runs(mc3)$call_variants) plotCWL(callVar) covert: To merge VCFs and convert to MAF: conv &lt;- readCWL(runs(mc3)$convert) plotCWL(conv) The merged VCF and converted MAF files will be collected to the output folder: outputs(mc3) ## List of length 2 ## names(2): outmaf outvcf 5.5.1 Prepare data Testing somatic mutation data can be download from: https://github.com/genome/somatic-snv-test-data. Input list inputList. The tumorID/normalID must be consistent with SM from BAM read group. inputList &lt;- list(tumorID=list(test=&quot;NA12892&quot;), normalID=list(test=&quot;NA12878&quot;), tumor=list(test=&quot;apps/DNASeq/data/tumor.bam&quot;), normal=list(test=&quot;apps/DNASeq/data/normal.bam&quot;)) Parameter list paramList. paramList &lt;- list(bed_file=&quot;apps/data/mc3/gaf_20111020+broad_wex_1.1_hg19.bed&quot;, centromere=&quot;apps/data/mc3/centromere_hg19.bed&quot;, cosmic=&quot;apps/data/mc3/hg19_cosmic_v54_120711.vcf.gz&quot;, dbsnp=&quot;apps/data/mc3/dbsnp_134_b37.leftAligned.vcf.gz&quot;, refFasta=&quot;apps/data/human_g1k_v37.fa.gz&quot;, vepData=&quot;apps/data/.vep/&quot;) 5.5.2 Run MC3 pipeline res &lt;- runCWLBatch(mc3, outdir = &quot;apps/DNASeq/output/mc3&quot;, inputList = inputList, paramList = paramList, BPPARAM = BatchtoolsParam(workers = 1, cluster = &quot;sge&quot;, template = &quot;apps/data/sge_centos7.tmpl&quot;, resources = list(threads = 2, queue = &quot;centos7.q&quot;))) The final VCF was filtered and merged from the outputs of different callers and annotated by VEP: dir(&quot;apps/DNASeq/output/mc3/test&quot;) ## [1] &quot;merged.vep.vcf&quot; &quot;vep.maf&quot; The merged VCF file was converted to MAF file: vcf &lt;- read.table(&quot;apps/DNASeq/output/mc3/test/merged.vep.vcf&quot;, sep=&quot;\\t&quot;) head(vcf) ## V1 V2 V3 V4 V5 V6 V7 ## 1 21 10400299 . A T 0.0 PASS ## 2 21 10400380 . C T . PASS ## 3 21 10402435 rs2948877 G A . PASS ## 4 21 10402715 . G A 0.0 PASS ## 5 21 10402795 rs148043841 G T . PASS ## 6 21 10402985 . G GA . PASS ## V8 ## 1 CENTERS=RADIA|VARSCANS|MUSE|SOMATICSNIPER;CSQ=T|intergenic_variant|MODIFIER|||||||||||||||rs370695467|1||||1|SNV|1||||||||||||||||||||||||||||||| ## 2 CENTERS=SOMATICSNIPER|RADIA|VARSCANS|MUSE;CSQ=T|intergenic_variant|MODIFIER||||||||||||||||1||||1|SNV|1||||||||||||||||||||||||||||||| ## 3 CENTERS=MUSE|RADIA|VARSCANS|SOMATICSNIPER;CSQ=A|intergenic_variant|MODIFIER|||||||||||||||rs2948877|1||||1|SNV|1||||||||||||||||A:0.3626|A:0.4266|A:0.3228|A:0.3075|A:0.2913|A:0.4346|||||||||| ## 4 CENTERS=RADIA|VARSCANS|MUSE;CSQ=A|intergenic_variant|MODIFIER|||||||||||||||rs2948878|1||||1|SNV|1||||||||||||||||||||||||||||||| ## 5 CENTERS=MUSE|RADIA|VARSCANS;CSQ=T|intergenic_variant|MODIFIER|||||||||||||||rs373568457|1||||1|SNV|1||||||||||||||||||||||||||||||| ## 6 CENTERS=VARSCANI*|PINDEL;CSQ=A|intergenic_variant|MODIFIER|||||||||||||||rs375209288|1||||1|insertion|1||||||||||||||||||||||||||||||| ## V9 V10 V11 ## 1 GT:DP:AD 0/0:140:140,0 0/1:92:71,20 ## 2 GT:DP:AD 0/0:160:160,0 0/1:117:99,18 ## 3 GT:DP:AD 0/0:167:167,0 0/1:124:97,27 ## 4 GT:DP:AD 0/0:145:145,0 0/1:117:97,20 ## 5 GT:DP:AD 0/0:163:161,2 0/1:127:108,19 ## 6 GT:DP:AD 0/0:88:88,0 0/1:82:75,7 5.6 GATK4 germline variant calling pipeline The GATK 4 best practice pipeline for germline variant calling was implemented with Workflow Description Language (WDL), which is similar to cwl and requires cromwell to run the pipeline. The details of the pipeline can be found here: https://software.broadinstitute.org/gatk/best-practices/workflow?id=11145 Germline short variant discovery (SNPs + Indels) The germline pipeline include 4 steps in WDL, paired fastq to ubam, GATK alignment, variant calling by HaplotypeCaller and joint genotyping. We wrapped the GATK pipeline into 3 steps using Rcwl for different numbers of computing nodes requirements. The wrapped pipeline can help to assign inputs to the input JSON templates and glob results from the cromwell outputs. GAlign GATK alignment. The fastqs, sample information and customized json files for WDL are required as inputs. Multiple steps will run in this step, including bwa alignment, mark duplicates and base quality recalibration. GATK ready BAM files will be collected into the output directory. hapCall HaplotypeCaller. The GATK ready BAM and customized json files are inputs in this step. The local paths of GATK bundle files are required to be modified in your json file. A “gVCF” files will be generated. jdCall Joint variant discovery This step will combine the “gVCF” files and then call germline variants in all samples. The paths of the local bundle files are also required to be changed in the json template file. The final VCF file of germline variants will be generated. 5.6.1 GATK Alignment We wrapped the steps from raw fastqs to analysis-ready BAM file into GAlign pipeline. Here is the short summary of the pipeline. data(GAlign) short(GAlign) ## inputs: ## - fastq1 ## - fastq2 ## - readGroup ## - sampleName ## - library ## - platunit ## - platform ## - center ## - tmpl1 ## - wdl1 ## - tmpl2 ## - wdl2 ## - cromwell ## outputs: ## - bamlog ## - outdir ## steps: ## - fqJson ## - fq2ubam ## - ubam2bamJson ## - align ## - mvOut For the inputList, we need to assign the fastqs files and read groups for each sample. The inputs can be multiple items separated by comma if there are more than one read groups for each sample. The input templates and WDL scripts can be assigned in the paramList, and the reference and other GATK bundle files in the local json files should be changed accordingly to your local version of files. The path to the cromwell binary file is also required. Here is an example: tmpl1 &lt;- system.file(package=&quot;RcwlPipelines&quot;, &quot;GATK4/seq-format-conversion/paired-fastq-to-unmapped-bam.inputs.json&quot;) tmpl2 &lt;- system.file(package=&quot;RcwlPipelines&quot;, &quot;GATK4/gatk4-data-processing/processing-for-variant-discovery-gatk4.hg38.wgs.inputs.local.json&quot;) wdl1 &lt;- system.file(package=&quot;RcwlPipelines&quot;, &quot;GATK4/seq-format-conversion/paired-fastq-to-unmapped-bam.wdl&quot;) wdl2 &lt;- system.file(package=&quot;RcwlPipelines&quot;, &quot;GATK4/gatk4-data-processing/processing-for-variant-discovery-gatk4.wdl&quot;) inputList &lt;- list(fastq1=list(normal=&quot;apps/DNASeq/data/normal_1.fq&quot;, tumor=&quot;apps/DNASeq/data/tumor_1.fq&quot;), fastq2=list(normal=&quot;apps/DNASeq/data/normal_2.fq&quot;, tumor=&quot;apps/DNASeq/data/tumor_2.fq&quot;), readGroup=list(&quot;normal.1&quot;, &quot;tumor.1&quot;), sampleName=list(&quot;normal&quot;, &quot;tumor&quot;), library=list(&quot;normal&quot;, &quot;tumor&quot;), platunit=list(&quot;normal&quot;, &quot;tumor&quot;), platform=list(&quot;illumina&quot;, &quot;illumina&quot;), center=list(&quot;rpccc&quot;, &quot;rpccc&quot;)) paramList &lt;- list(tmpl1=tmpl1, wdl1=wdl1, tmpl2=tmpl2, wdl2=wdl2, cromwell=&quot;/software/cromwell-36.jar&quot;) r1 &lt;- runCWLBatch(GAlign, outdir=&quot;apps/DNASeq/output/BAM&quot;, inputList, paramList, BatchtoolsParam(workers = 2, cluster=&quot;sge&quot;, template = &quot;apps/data/sge_centos7.tmpl&quot;, resources = list(threads = 16, queue = &quot;centos7.q&quot;)), stderr=&quot;&quot;) The outputs were globbed from the cromwell execution folder: list.files(&quot;apps/DNASeq/output/BAM/normal&quot;, recursive = TRUE) ## [1] &quot;output/normal.hg38.bai&quot; ## [2] &quot;output/normal.hg38.bam&quot; ## [3] &quot;output/normal.hg38.bam.md5&quot; ## [4] &quot;output/normal.hg38.duplicate_metrics&quot; ## [5] &quot;output/normal.hg38.recal_data.csv&quot; ## [6] &quot;processing-for-variant-discovery-gatk4.wdl.log&quot; 5.6.2 HaplotypeCaller This step takes the BAM files as input and each BAM file will be assigned to different computing nodes. The json template file needs to be modified to include the correct GATK bundle paths first. data(hapCall) wdl3 &lt;- system.file(package=&quot;RcwlPipelines&quot;, &quot;GATK4/gatk4-germline-snps-indels/haplotypecaller-gvcf-gatk4.wdl&quot;) tmpl3 &lt;- system.file(package=&quot;RcwlPipelines&quot;, &quot;GATK4/gatk4-germline-snps-indels/haplotypecaller-gvcf-gatk4.hg38.inputs.local.json&quot;) bams &lt;- list(normal = normalizePath(&quot;output/BAM/normal/output/normal.hg38.bam&quot;), tumor = normalizePath(&quot;output/BAM/tumor/output/tumor.hg38.bam&quot;)) inputList &lt;- list(bam = bams) paramList &lt;- list(intervals = normalizePath(&quot;output/interval.txt&quot;), cromwell = &quot;/software/cromwell-36.jar&quot;, wdl = wdl3, tmpl = tmpl3) r2 &lt;- runCWLBatch(hapCall, outdir=&quot;apps/DNASeq/output/GATK&quot;, inputList, paramList, BatchtoolsParam(workers = 2, cluster=&quot;sge&quot;, template = &quot;apps/data/sge_centos7.tmpl&quot;, resources = list(threads = 16, queue = &quot;centos7.q&quot;)), stderr=&quot;&quot;) Here are the outputs: list.files(&quot;apps/DNASeq/output/GATK/normal&quot;, recursive = TRUE) ## [1] &quot;haplotypecaller-gvcf-gatk4.wdl.log&quot; ## [2] &quot;output/normal.hg38.g.vcf.gz&quot; ## [3] &quot;output/normal.hg38.g.vcf.gz.tbi&quot; 5.6.3 Joint Discovery The joint genotyping step will combine the gvcf files and then call variants in all samples, so only one computing node is required. Multiple values or files of the samples will need to be seperated by comma for each input in the inputList. The paths of the local bundle files will also need to be added to the json template file. data(jdCall) wdl4 &lt;- system.file(package=&quot;RcwlPipelines&quot;, &quot;GATK4/gatk4-germline-snps-indels/joint-discovery-gatk4-local.wdl&quot;) tmpl4 &lt;- system.file(package=&quot;RcwlPipelines&quot;, &quot;GATK4/gatk4-germline-snps-indels/joint-discovery-gatk4-local.hg38.wgs.inputs.json&quot;) inputList &lt;- list(sampleName = list(test=&quot;normal,tumor&quot;), gvcf = list(test=&quot;apps/DNASeq/output/GATK/normal/output/normal.hg38.g.vcf.gz,apps/DNASeq/output/GATK/tumor/output/tumor.hg38.g.vcf.gz&quot;)) paramList &lt;- list(callsetName = &quot;test&quot;, intervals = &quot;apps/DNASeq/output/interval.21.interval_list&quot;, unpadded_intervals = &quot;apps/DNASeq/output/interval.21.intervals&quot;, tmpl = tmpl4, cromwell = &quot;/software/cromwell-36.jar&quot;, wdl = wdl4) r3 &lt;- runCWLBatch(jdCall, outdir=&quot;apps/DNASeq/output/GATK&quot;, inputList, paramList, BatchtoolsParam(workers = 1, cluster=&quot;sge&quot;, template = &quot;apps/sge_centos7.tmpl&quot;, resources = list(threads = 16, queue = &quot;centos7.q&quot;)), stderr=&quot;&quot;) Here are the final outputs: list.files(&quot;apps/DNASeq/output/GATK/test&quot;, recursive = TRUE) ## [1] &quot;joint-discovery-gatk4-local.wdl.log&quot; ## [2] &quot;output/out.intervals&quot; ## [3] &quot;output/test.variant_calling_detail_metrics&quot; ## [4] &quot;output/test.variant_calling_summary_metrics&quot; ## [5] &quot;output/test.vcf.gz&quot; ## [6] &quot;output/test.vcf.gz.tbi&quot; "],
["references.html", "References", " References "]
]
